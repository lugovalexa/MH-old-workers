{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../\")\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from pandas.io.stata import StataReader\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from utils.retirement import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and merge all needed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/Users/alexandralugova/Documents/GitHub/MH-old-workers/data/datasets/sharew7_rel8-0-0_ALL_datasets_stata/\"\n",
    "merge_columns = [\"mergeid\", \"hhid7\", \"mergeidp7\", \"coupleid7\", \"country\", \"language\"]\n",
    "datasets = []\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if (\n",
    "        filename.endswith(\"cv_r.dta\")\n",
    "        or filename.endswith(\"technical_variables.dta\")\n",
    "        or filename.endswith(\"dn.dta\")\n",
    "        # or filename.endswith(\"ra.dta\")\n",
    "        # or filename.endswith(\"cc.dta\")\n",
    "        # or filename.endswith(\"dq.dta\")\n",
    "        or filename.endswith(\"fs.dta\")\n",
    "        # or filename.endswith(\"gl.dta\")\n",
    "        or filename.endswith(\"rh.dta\")\n",
    "        or filename.endswith(\"hs.dta\")\n",
    "        or filename.endswith(\"rc.dta\")\n",
    "        or filename.endswith(\"re.dta\")\n",
    "        or filename.endswith(\"rp.dta\")\n",
    "        or filename.endswith(\"wq.dta\")\n",
    "        or filename.endswith(\"gv_weights.dta\")\n",
    "    ):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        dataset = pd.read_stata(file_path, convert_categoricals=False)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=merge_columns), datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only Sharelife part\n",
    "df = df[df.mn103_ == 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63248"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform some variables and choose only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "Estonia           5115\n",
       "Slovenia          3691\n",
       "Poland            3559\n",
       "Spain             3424\n",
       "Belgium           3333\n",
       "Czech Republic    3292\n",
       "Italy             3000\n",
       "Germany           2984\n",
       "Austria           2693\n",
       "Croatia           2408\n",
       "France            2188\n",
       "Israel            2131\n",
       "Sweden            2130\n",
       "Romania           2114\n",
       "Slovakia          2077\n",
       "Lithuania         2035\n",
       "Finland           2007\n",
       "Bulgaria          1998\n",
       "Denmark           1962\n",
       "Latvia            1734\n",
       "Switzerland       1648\n",
       "Hungary           1538\n",
       "Portugal          1282\n",
       "Malta             1261\n",
       "Luxembourg        1250\n",
       "Cyprus            1233\n",
       "Greece            1161\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform countries from codes to names\n",
    "with StataReader(\n",
    "    \"/Users/alexandralugova/Documents/GitHub/MH-old-workers/data/datasets/sharew7_rel8-0-0_ALL_datasets_stata/sharew7_rel8-0-0_cv_r.dta\",\n",
    "    convert_categoricals=True,\n",
    ") as reader:\n",
    "    data = reader.read()\n",
    "    value_labels = reader.value_labels()\n",
    "df[\"country\"] = df[\"country\"].replace(value_labels.get(\"country\"))\n",
    "\n",
    "df[\"country\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "1    36040\n",
       "0    27208\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform gender to 1=female, 0=male\n",
    "df[\"gender\"] = df[\"gender\"].replace({1: 0, 2: 1})\n",
    "\n",
    "df[\"gender\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partnerinhh\n",
       "1    45347\n",
       "0    17901\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform partnerinhh to 1=lives with partner and 0=without\n",
    "df[\"partnerinhh\"] = df[\"partnerinhh\"].replace({3: 0})\n",
    "\n",
    "df[\"partnerinhh\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    63233.000000\n",
       "mean      1949.768934\n",
       "std         10.511566\n",
       "min       1912.000000\n",
       "25%       1943.000000\n",
       "50%       1951.000000\n",
       "75%       1958.000000\n",
       "max       2017.000000\n",
       "Name: yr1country, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename variable indicating first year in country and fill missing values with year of birth\n",
    "df[\"dn006_\"] = df[\"dn006_\"].fillna(df[\"yrbirth\"])\n",
    "df = df.rename(columns={\"dn006_\": \"yr1country\"})\n",
    "\n",
    "# Drop individuals with missing answers\n",
    "df = df[df.yr1country > 0].reset_index(drop=True)\n",
    "\n",
    "df[\"yr1country\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    62153.000000\n",
       "mean        11.041068\n",
       "std          4.415459\n",
       "min          0.000000\n",
       "25%          8.000000\n",
       "50%         11.000000\n",
       "75%         13.000000\n",
       "max         40.000000\n",
       "Name: yrseducation, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the number of education years\n",
    "waves = [1, 2, 4, 5, 6, 7]\n",
    "dfs = []\n",
    "\n",
    "for wave in waves:\n",
    "    file_path = f\"/Users/alexandralugova/Documents/GitHub/MH-old-workers/data/datasets/sharew{wave}_rel8-0-0_ALL_datasets_stata/sharew{wave}_rel8-0-0_dn.dta\"\n",
    "    data = pd.read_stata(file_path, convert_categoricals=False)\n",
    "    dfs.append(data)\n",
    "\n",
    "dn_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "edu_sum = dn_data.groupby(\"mergeid\").dn041_.sum().to_frame().reset_index()\n",
    "edu_sum = edu_sum[(edu_sum.dn041_ >= 0) & (edu_sum.dn041_ <= 40)].reset_index(drop=True)\n",
    "edu_sum = edu_sum.rename(columns={\"dn041_\": \"yrseducation\"})\n",
    "\n",
    "df = df.merge(edu_sum, on=\"mergeid\", how=\"left\")\n",
    "\n",
    "# Drop individuals with missing values\n",
    "df = df.dropna(subset=\"yrseducation\").reset_index(drop=True)\n",
    "\n",
    "df.yrseducation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    62112.000000\n",
       "mean         2.059892\n",
       "std          1.333923\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          3.000000\n",
       "max         16.000000\n",
       "Name: nb_children, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop individuals with missing values for number of children (drop if refusal and 0 if nan)\n",
    "df[[\"rc023_\", \"rc039_\"]] = df[[\"rc023_\", \"rc039_\"]].fillna(0)\n",
    "df = df[df.rc023_ >= 0].reset_index(drop=True)\n",
    "\n",
    "# Calculate the number of children as sum of biological and adopted\n",
    "df[\"nb_children\"] = df[\"rc023_\"] + df[\"rc039_\"]\n",
    "\n",
    "df[\"nb_children\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isco\n",
       "4110.0    1447\n",
       "5223.0    1249\n",
       "9112.0     873\n",
       "2221.0     776\n",
       "2341.0     716\n",
       "          ... \n",
       "2122.0       1\n",
       "5161.0       1\n",
       "8334.0       1\n",
       "2358.0       1\n",
       "3514.0       1\n",
       "Name: count, Length: 449, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify current job isco\n",
    "isco_columns = [f\"re012isco_{i}\" for i in range(1, 21)]\n",
    "\n",
    "\n",
    "def get_last_valid(row):\n",
    "    last_valid_index = row.last_valid_index()\n",
    "    if pd.notnull(last_valid_index):\n",
    "        return row[last_valid_index]\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "df[\"isco\"] = df[isco_columns].apply(get_last_valid, axis=1)\n",
    "\n",
    "# Drop individuals with missing values\n",
    "df = df[df.isco > 0].reset_index(drop=True)\n",
    "df = df.dropna(subset=\"isco\").reset_index(drop=True)\n",
    "\n",
    "df[\"isco\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 46517 entries, 0 to 46516\n",
      "Series name: isco\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "46517 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 363.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df[\"isco\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         value \u001b[39m=\u001b[39m value[:\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m value[\u001b[39m3\u001b[39m:]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m'\u001b[39;49m\u001b[39misco\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(modify_isco)\u001b[39m.\u001b[39munique()\n",
      "File \u001b[0;32m~/miniconda3/envs/mhold/lib/python3.11/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/miniconda3/envs/mhold/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/mhold/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/mhold/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/miniconda3/envs/mhold/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodify_isco\u001b[39m(value):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(value) \u001b[39m<\u001b[39m \u001b[39m6\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         value \u001b[39m=\u001b[39m value[:\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m value[\u001b[39m3\u001b[39m:]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandralugova/Documents/GitHub/MH-old-workers/notebooks/creating_dataset_sharelife.ipynb#Y201sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "def modify_isco(value):\n",
    "    if len(value) < 6:\n",
    "        value = value[:3] + \"0\" + value[3:]\n",
    "    return value\n",
    "\n",
    "\n",
    "df[\"isco\"].apply(modify_isco).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mergeid</th>\n",
       "      <th>hhid7</th>\n",
       "      <th>mergeidp7</th>\n",
       "      <th>coupleid7</th>\n",
       "      <th>country</th>\n",
       "      <th>language</th>\n",
       "      <th>fs002_</th>\n",
       "      <th>fs003_</th>\n",
       "      <th>fs004_</th>\n",
       "      <th>fs005_</th>\n",
       "      <th>...</th>\n",
       "      <th>hs063d2</th>\n",
       "      <th>hs063d3</th>\n",
       "      <th>hs063d4</th>\n",
       "      <th>hs063d5</th>\n",
       "      <th>hs063d6</th>\n",
       "      <th>hs063d7</th>\n",
       "      <th>hs063dno</th>\n",
       "      <th>hs063dot</th>\n",
       "      <th>nb_children</th>\n",
       "      <th>isco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT-057726-04</td>\n",
       "      <td>AT-057726-A</td>\n",
       "      <td>AT-057726-01</td>\n",
       "      <td>AT-057726-01-04</td>\n",
       "      <td>Austria</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT-059165-01</td>\n",
       "      <td>AT-059165-A</td>\n",
       "      <td>AT-059165-02</td>\n",
       "      <td>AT-059165-01-02</td>\n",
       "      <td>Austria</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT-079798-01</td>\n",
       "      <td>AT-079798-A</td>\n",
       "      <td>AT-079798-02</td>\n",
       "      <td>AT-079798-01-02</td>\n",
       "      <td>Austria</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT-084096-02</td>\n",
       "      <td>AT-084096-A</td>\n",
       "      <td>AT-084096-01</td>\n",
       "      <td>AT-084096-01-02</td>\n",
       "      <td>Austria</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT-094882-01</td>\n",
       "      <td>AT-094882-A</td>\n",
       "      <td>AT-094882-02</td>\n",
       "      <td>AT-094882-01-02</td>\n",
       "      <td>Austria</td>\n",
       "      <td>11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>SK-995042-01</td>\n",
       "      <td>SK-995042-A</td>\n",
       "      <td>SK-995042-02</td>\n",
       "      <td>SK-995042-01-02</td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>SK-995042-02</td>\n",
       "      <td>SK-995042-A</td>\n",
       "      <td>SK-995042-01</td>\n",
       "      <td>SK-995042-01-02</td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>SK-996004-01</td>\n",
       "      <td>SK-996004-A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>SK-999958-01</td>\n",
       "      <td>SK-999958-A</td>\n",
       "      <td>SK-999958-02</td>\n",
       "      <td>SK-999958-01-02</td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>SK-999958-02</td>\n",
       "      <td>SK-999958-A</td>\n",
       "      <td>SK-999958-01</td>\n",
       "      <td>SK-999958-01-02</td>\n",
       "      <td>Slovakia</td>\n",
       "      <td>63</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7531.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12020 rows Ã— 1725 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mergeid        hhid7     mergeidp7        coupleid7   country  \\\n",
       "0      AT-057726-04  AT-057726-A  AT-057726-01  AT-057726-01-04   Austria   \n",
       "1      AT-059165-01  AT-059165-A  AT-059165-02  AT-059165-01-02   Austria   \n",
       "2      AT-079798-01  AT-079798-A  AT-079798-02  AT-079798-01-02   Austria   \n",
       "3      AT-084096-02  AT-084096-A  AT-084096-01  AT-084096-01-02   Austria   \n",
       "4      AT-094882-01  AT-094882-A  AT-094882-02  AT-094882-01-02   Austria   \n",
       "...             ...          ...           ...              ...       ...   \n",
       "12015  SK-995042-01  SK-995042-A  SK-995042-02  SK-995042-01-02  Slovakia   \n",
       "12016  SK-995042-02  SK-995042-A  SK-995042-01  SK-995042-01-02  Slovakia   \n",
       "12017  SK-996004-01  SK-996004-A                                 Slovakia   \n",
       "12018  SK-999958-01  SK-999958-A  SK-999958-02  SK-999958-01-02  Slovakia   \n",
       "12019  SK-999958-02  SK-999958-A  SK-999958-01  SK-999958-01-02  Slovakia   \n",
       "\n",
       "       language  fs002_  fs003_  fs004_  fs005_  ...  hs063d2  hs063d3  \\\n",
       "0            11     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "1            11     1.0  1990.0     5.0     NaN  ...      NaN      NaN   \n",
       "2            11     1.0  1995.0     5.0     NaN  ...      NaN      NaN   \n",
       "3            11     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "4            11     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "...         ...     ...     ...     ...     ...  ...      ...      ...   \n",
       "12015        63     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "12016        63     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "12017        63     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "12018        63     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "12019        63     5.0     NaN     5.0     NaN  ...      NaN      NaN   \n",
       "\n",
       "       hs063d4  hs063d5  hs063d6  hs063d7  hs063dno  hs063dot  nb_children  \\\n",
       "0          NaN      NaN      NaN      NaN       NaN       NaN          0.0   \n",
       "1          NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "2          NaN      NaN      NaN      NaN       NaN       NaN          2.0   \n",
       "3          NaN      NaN      NaN      NaN       NaN       NaN          1.0   \n",
       "4          NaN      NaN      NaN      NaN       NaN       NaN          5.0   \n",
       "...        ...      ...      ...      ...       ...       ...          ...   \n",
       "12015      NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "12016      NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "12017      NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "12018      NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "12019      NaN      NaN      NaN      NaN       NaN       NaN          3.0   \n",
       "\n",
       "         isco  \n",
       "0      5414.0  \n",
       "1      5221.0  \n",
       "2      3431.0  \n",
       "3      3313.0  \n",
       "4      6330.0  \n",
       "...       ...  \n",
       "12015  9211.0  \n",
       "12016  8211.0  \n",
       "12017  4414.0  \n",
       "12018  8342.0  \n",
       "12019  7531.0  \n",
       "\n",
       "[12020 rows x 1725 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[\n",
    "    [\n",
    "        \"mergeid\",\n",
    "        \"country\",\n",
    "        \"firstwave\",\n",
    "        \"gender\",\n",
    "        \"yrbirth\",\n",
    "        \"mobirth\",\n",
    "        \"age2017\",\n",
    "        \"yr1country\",\n",
    "        \"partnerinhh\",\n",
    "        \"highest_education\",\n",
    "        \"nb_children\",\n",
    "        \"isco\",\n",
    "        \"ep009_\",\n",
    "        \"ep012_\",\n",
    "        \"ep016_\",\n",
    "        \"ep616isco\",\n",
    "        # \"ep026_\",\n",
    "        # \"ep027_\",\n",
    "        # \"ep028_\",\n",
    "        # \"ep029_\",\n",
    "        # \"ep030_\",\n",
    "        # \"ep031_\",\n",
    "        # \"ep032_\",\n",
    "        # \"ep033_\",\n",
    "        # \"ep034_\",\n",
    "        # \"ep035_\",\n",
    "        # \"ep036_\",\n",
    "        # \"ep037_\",\n",
    "        \"ep071dno\",\n",
    "        \"ep671dno\",\n",
    "        \"euro1\",\n",
    "        \"euro2\",\n",
    "        \"euro3\",\n",
    "        \"euro4\",\n",
    "        \"euro5\",\n",
    "        \"euro6\",\n",
    "        \"euro7\",\n",
    "        \"euro8\",\n",
    "        \"euro9\",\n",
    "        \"euro10\",\n",
    "        \"euro11\",\n",
    "        \"euro12\",\n",
    "        \"eurod\",\n",
    "        \"eurodcat\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename some essential columns\n",
    "df_short = df_short.rename(\n",
    "    columns={\n",
    "        \"dn006_\": \"yr1country\",\n",
    "        \"dn010_\": \"highest_education\",\n",
    "        \"ch001_\": \"nb_children\",\n",
    "        \"ep005_\": \"employment\",\n",
    "        \"ep009_\": \"job_status\",\n",
    "        \"ep012_\": \"job_week_hours\",\n",
    "        \"ep016_\": \"job_name\",\n",
    "        \"ep616isco\": \"job_isco\",\n",
    "        \"ep071dno\": \"pension\",\n",
    "        \"ep671dno\": \"pension1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len wave 1: 30419, Missing isco: 30419\n",
      "Len wave 2: 37143, Missing isco: 37143\n",
      "Len wave 4: 58000, Missing isco: 58000\n",
      "Len wave 5: 66065, Missing isco: 66065\n",
      "Len wave 6: 68085, Missing isco: 61670\n",
      "Len wave 8: 46733, Missing isco: 42036\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 4, 5, 6, 8]:\n",
    "    print(\n",
    "        f\"Len wave {i}: {len(df_short[df_short.wave == i])}, Missing isco: {df_short[df_short.wave == i]['job_isco'].isna().sum()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave1[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave1[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave1[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave1[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/1079895796.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n"
     ]
    }
   ],
   "source": [
    "# Keep only countries present in both waves in each couple\n",
    "filtered_countries = []\n",
    "\n",
    "wave_pairs = [(1, 2), (2, 4), (4, 5), (5, 6), (6, 8)]\n",
    "\n",
    "for wave1, wave2 in wave_pairs:\n",
    "    wave1_data = df_short[df_short[\"wave\"] == wave1]\n",
    "    wave2_data = df_short[df_short[\"wave\"] == wave2]\n",
    "\n",
    "    common_countries = set(wave1_data[\"country\"]) & set(wave2_data[\"country\"])\n",
    "\n",
    "    common_countries_wave1 = wave1_data[wave1_data[\"country\"].isin(common_countries)]\n",
    "    common_countries_wave2 = wave2_data[wave2_data[\"country\"].isin(common_countries)]\n",
    "\n",
    "    common_countries_wave1[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
    "    common_countries_wave2[\"wave_couple\"] = f\"w{wave1}{wave2}\"\n",
    "\n",
    "    filtered_countries.append(common_countries_wave1)\n",
    "    filtered_countries.append(common_countries_wave2)\n",
    "\n",
    "df_filtered = pd.concat(filtered_countries, sort=False, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only 15 principal countries of interest\n",
    "countries_list = [\n",
    "    \"Austria\",\n",
    "    \"Belgium\",\n",
    "    \"Czech Republic\",\n",
    "    \"Denmark\",\n",
    "    \"Estonia\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Italy\",\n",
    "    \"Luxembourg\",\n",
    "    \"Netherlands\",\n",
    "    \"Poland\",\n",
    "    \"Portugal\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Switzerland\",\n",
    "]\n",
    "\n",
    "df_filtered = df_filtered[df_filtered[\"country\"].isin(countries_list)].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering we have 14 out of 29 countries left. They are: ['Austria' 'Belgium' 'Switzerland' 'Germany' 'Denmark' 'Spain' 'France'\n",
      " 'Italy' 'Netherlands' 'Czech Republic' 'Poland' 'Estonia' 'Slovenia'\n",
      " 'Luxembourg']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"After filtering we have {df_filtered.country.nunique()} out of {df_short.country.nunique()} countries left. They are: {df_filtered.country.unique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w12 - 9 common countries:\n",
      "['Austria' 'Belgium' 'Switzerland' 'Germany' 'Denmark' 'Spain' 'France'\n",
      " 'Italy' 'Netherlands']\n",
      "w24 - 11 common countries:\n",
      "['Austria' 'Belgium' 'Czech Republic' 'Switzerland' 'Germany' 'Denmark'\n",
      " 'Spain' 'France' 'Italy' 'Netherlands' 'Poland']\n",
      "w45 - 12 common countries:\n",
      "['Austria' 'Belgium' 'Czech Republic' 'Switzerland' 'Germany' 'Denmark'\n",
      " 'Estonia' 'Spain' 'France' 'Italy' 'Netherlands' 'Slovenia']\n",
      "w56 - 12 common countries:\n",
      "['Austria' 'Belgium' 'Czech Republic' 'Switzerland' 'Germany' 'Denmark'\n",
      " 'Estonia' 'Spain' 'France' 'Italy' 'Luxembourg' 'Slovenia']\n",
      "w68 - 13 common countries:\n",
      "['Austria' 'Belgium' 'Czech Republic' 'Switzerland' 'Germany' 'Denmark'\n",
      " 'Estonia' 'Spain' 'France' 'Italy' 'Luxembourg' 'Poland' 'Slovenia']\n"
     ]
    }
   ],
   "source": [
    "for couple in df_filtered.wave_couple.unique():\n",
    "    print(\n",
    "        f\"{couple} - {df_filtered[df_filtered.wave_couple == couple].country.nunique()} common countries:\"\n",
    "    )\n",
    "    print(df_filtered[df_filtered.wave_couple == couple][\"country\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1    22024\n",
       "2    27454\n",
       "4    50946\n",
       "5    58911\n",
       "6    53052\n",
       "8    28587\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for only aged 50-67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_to_year = {1: 2004, 2: 2007, 4: 2011, 5: 2013, 6: 2015, 8: 2020}\n",
    "\n",
    "# Year of survey\n",
    "df_filtered[\"yrsurvey\"] = df_filtered[\"wave\"].map(wave_to_year).astype(int)\n",
    "\n",
    "# Year of birth\n",
    "df_filtered[\"yrbirth\"] = pd.to_numeric(df_filtered[\"yrbirth\"], errors=\"coerce\")\n",
    "df_filtered = df_filtered[df_filtered[\"yrbirth\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# Age\n",
    "df_filtered[\"age\"] = df_filtered[\"yrsurvey\"] - df_filtered[\"yrbirth\"]\n",
    "\n",
    "# Filter for 50+\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered[\"age\"] >= 50) & (df_filtered[\"age\"] <= 67)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1    13403\n",
       "2    16230\n",
       "4    28745\n",
       "5    32318\n",
       "6    27299\n",
       "8    10320\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for not retired and employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[\n",
    "    df_filtered.employment\n",
    "    == \"Employed or self-employed (including working for family business)\"\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1     5349\n",
       "2     6909\n",
       "4    12662\n",
       "5    14852\n",
       "6    12355\n",
       "8     4329\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out those who hold state pensions for disabilities or other special conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep071dno and ep671dno (from wave 6) - no current state pensions\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered.pension == \"Selected\") | (df_filtered.pension1 == \"Selected\")\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1     4888\n",
       "2     6359\n",
       "4    11158\n",
       "5    13167\n",
       "6    10832\n",
       "8     3658\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate contribution years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job episodes panel data (from retrospective waves 3 and 7)\n",
    "jobs = pd.read_stata(\n",
    "    \"/Users/alexandralugova/Documents/GitHub/MH-old-workers/data/datasets/sharewX_rel8-0-0_gv_job_episodes_panel.dta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of years of work for each individual\n",
    "conditions = [\"Employee or self-employed\", \"Short term job (less than 6 months)\"]\n",
    "relevant_rows = jobs[jobs[\"situation\"].isin(conditions)]\n",
    "result_jobs = (\n",
    "    relevant_rows.groupby(\"mergeid\").size().reset_index(name=\"yrscontribution2019\")\n",
    ")\n",
    "# Calculate the year of first contribution\n",
    "first_contribution = (\n",
    "    relevant_rows.groupby(\"mergeid\")[\"year\"].min().reset_index(name=\"yr1contribution\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with main dataset\n",
    "df_filtered = df_filtered.merge(result_jobs, on=\"mergeid\", how=\"left\")\n",
    "df_filtered[\"yrscontribution\"] = df_filtered[\"yrscontribution2019\"] - (\n",
    "    2019 - df_filtered[\"yrsurvey\"]\n",
    ")\n",
    "df_filtered = df_filtered.merge(first_contribution, on=\"mergeid\", how=\"left\")\n",
    "df_filtered = df_filtered.merge(\n",
    "    jobs[[\"mergeid\", \"year\", \"withpartner\"]].rename(columns={\"year\": \"yrsurvey\"}),\n",
    "    on=[\"mergeid\", \"yrsurvey\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Delete those with less than 10 years of contributions\n",
    "df_filtered = df_filtered[df_filtered[\"yrscontribution\"] >= 10]\n",
    "# Delete those who started work before the age of 10\n",
    "df_filtered = df_filtered[\n",
    "    df_filtered[\"yr1contribution\"].astype(int)\n",
    "    >= df_filtered[\"yrbirth\"].astype(int) + 12\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1    2935\n",
       "2    4651\n",
       "4    7238\n",
       "5    8749\n",
       "6    8569\n",
       "8    3370\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set legal retirement ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some necessary formatting\n",
    "month_to_numeric = {\n",
    "    \"January\": 1,\n",
    "    \"February\": 2,\n",
    "    \"March\": 3,\n",
    "    \"April\": 4,\n",
    "    \"May\": 5,\n",
    "    \"June\": 6,\n",
    "    \"July\": 7,\n",
    "    \"August\": 8,\n",
    "    \"September\": 9,\n",
    "    \"October\": 10,\n",
    "    \"November\": 11,\n",
    "    \"December\": 12,\n",
    "}\n",
    "\n",
    "df_filtered[\"mbirth\"] = df_filtered[\"dn002_\"].map(month_to_numeric)\n",
    "df_filtered[\"yr1country\"] = df_filtered[\"yr1country\"].fillna(df_filtered[\"yrbirth\"])\n",
    "df_filtered = df_filtered[\n",
    "    ~(\n",
    "        (df_filtered[\"country\"] == \"Czech Republic\")\n",
    "        & (df_filtered[\"gender\"] == \"Female\")\n",
    "        & pd.to_numeric(df_filtered[\"nb_children\"], errors=\"coerce\").isna()\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_functions_age = {\n",
    "    \"Austria\": austria_age,\n",
    "    \"Belgium\": belgium_age,\n",
    "    \"Czech Republic\": czech_republic_age,\n",
    "    \"Denmark\": denmark_age,\n",
    "    \"Estonia\": estonia_age,\n",
    "    \"France\": france_age,\n",
    "    \"Germany\": germany_age,\n",
    "    \"Italy\": italy_age,\n",
    "    \"Luxembourg\": luxembourg_age,\n",
    "    \"Netherlands\": netherlands_age,\n",
    "    \"Poland\": poland_age,\n",
    "    \"Slovenia\": slovenia_age,\n",
    "    \"Spain\": spain_age,\n",
    "    \"Switzerland\": switzerland_age,\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_retirement_age(row):\n",
    "    country = row[\"country\"]\n",
    "    if country in country_functions_age:\n",
    "        return country_functions_age[country](row)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply country-wise functions to calculate legal retirement age\n",
    "df_filtered[\"retirement_age\"] = df_filtered.apply(calculate_retirement_age, axis=1)\n",
    "\n",
    "# Delete those who are above the retirement age (continue to work longer)\n",
    "df_filtered = df_filtered[\n",
    "    df_filtered[\"retirement_age\"] > df_filtered[\"age\"]\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1    2891\n",
       "2    4483\n",
       "4    6864\n",
       "5    8383\n",
       "6    8196\n",
       "8    3080\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate resting work horizon and its change due to reforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resting work horizon\n",
    "df_filtered[\"work_horizon\"] = df_filtered[\"retirement_age\"] - df_filtered[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_functions_change = {\n",
    "    \"Austria\": austria_change,\n",
    "    \"Belgium\": belgium_change,\n",
    "    \"Czech Republic\": czech_republic_change,\n",
    "    \"Denmark\": denmark_change,\n",
    "    \"Estonia\": estonia_change,\n",
    "    \"France\": france_change,\n",
    "    \"Germany\": germany_change,\n",
    "    \"Italy\": italy_change,\n",
    "    \"Luxembourg\": luxembourg_change,\n",
    "    \"Netherlands\": netherlands_change,\n",
    "    \"Poland\": poland_change,\n",
    "    \"Slovenia\": slovenia_change,\n",
    "    \"Spain\": spain_change,\n",
    "    \"Switzerland\": switzerland_change,\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_horizon_change(row):\n",
    "    country = row[\"country\"]\n",
    "    if country in country_functions_change:\n",
    "        return country_functions_change[country](row)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply country-wise functions to calculate work horizon change due to reforms\n",
    "df_filtered[\"work_horizon_change\"] = df_filtered.apply(calculate_horizon_change, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mental health indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out those with missing values for eurod scale\n",
    "df_filtered = df_filtered.dropna(subset=[\"eurod\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wave\n",
       "1    2861\n",
       "2    4431\n",
       "4    6752\n",
       "5    8253\n",
       "6    8006\n",
       "8    3052\n",
       "Name: mergeid, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique individuals\n",
    "df_filtered.groupby(\"wave\").mergeid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/3npmnx597nb730qdy5w_6kp00000gn/T/ipykernel_38808/889948878.py:36: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  ].applymap(\n"
     ]
    }
   ],
   "source": [
    "# Transform to numeric\n",
    "df_filtered[\"eurod\"] = df_filtered[\"eurod\"].replace(\n",
    "    {\"Not depressed\": 0, \"Very depressed\": 12}\n",
    ")\n",
    "df_filtered[\"eurodcat\"] = df_filtered[\"eurodcat\"].replace({\"Yes\": 1, \"No\": 0})\n",
    "df_filtered[\n",
    "    [\n",
    "        \"euro1\",\n",
    "        \"euro2\",\n",
    "        \"euro3\",\n",
    "        \"euro4\",\n",
    "        \"euro5\",\n",
    "        \"euro6\",\n",
    "        \"euro7\",\n",
    "        \"euro8\",\n",
    "        \"euro9\",\n",
    "        \"euro10\",\n",
    "        \"euro11\",\n",
    "        \"euro12\",\n",
    "    ]\n",
    "] = df_filtered[\n",
    "    [\n",
    "        \"euro1\",\n",
    "        \"euro2\",\n",
    "        \"euro3\",\n",
    "        \"euro4\",\n",
    "        \"euro5\",\n",
    "        \"euro6\",\n",
    "        \"euro7\",\n",
    "        \"euro8\",\n",
    "        \"euro9\",\n",
    "        \"euro10\",\n",
    "        \"euro11\",\n",
    "        \"euro12\",\n",
    "    ]\n",
    "].applymap(\n",
    "    lambda x: 1 if x == \"Selected\" else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a PCA to deduct affective suffering and lack of motivation as separate indicators\n",
    "columns_for_pca = [\n",
    "    \"euro1\",\n",
    "    \"euro2\",\n",
    "    \"euro3\",\n",
    "    \"euro4\",\n",
    "    \"euro5\",\n",
    "    \"euro6\",\n",
    "    \"euro7\",\n",
    "    \"euro8\",\n",
    "    \"euro9\",\n",
    "    \"euro10\",\n",
    "    \"euro11\",\n",
    "    \"euro12\",\n",
    "]\n",
    "data_pca = df_filtered[columns_for_pca]\n",
    "\n",
    "corr_mat = np.corrcoef(data_pca, rowvar=False)  # Tetrachoric correlation matrix\n",
    "\n",
    "evals, evecs = eigh(corr_mat)  # Eigenvalues and eigenvectors\n",
    "\n",
    "fa = FactorAnalyzer(n_factors=2, rotation=\"varimax\", method=\"ml\")\n",
    "fa.fit(data_pca)\n",
    "\n",
    "factor_scores = fa.transform(data_pca)\n",
    "\n",
    "cutoff = 0.55\n",
    "\n",
    "df_filtered[\"affective_suffering\"] = 0\n",
    "df_filtered[\"motivation_lack\"] = 0\n",
    "\n",
    "df_filtered[\"affective_suffering\"] = (factor_scores[:, 0] >= cutoff).astype(int)\n",
    "df_filtered[\"motivation_lack\"] = (factor_scores[:, 1] >= cutoff).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72230676, 0.06609387],\n",
       "       [0.01712821, 0.21397277],\n",
       "       [0.2376367 , 0.30820253],\n",
       "       [0.26465848, 0.21428754],\n",
       "       [0.39408909, 0.17606154],\n",
       "       [0.17611074, 0.38047347],\n",
       "       [0.40292027, 0.19079989],\n",
       "       [0.19200065, 0.25428392],\n",
       "       [0.36061593, 0.2733768 ],\n",
       "       [0.16281977, 0.32561402],\n",
       "       [0.0287768 , 0.21387176],\n",
       "       [0.48854601, 0.06975363]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.loadings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affective_suffering\n",
       "0    40948\n",
       "1    17282\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.affective_suffering.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "motivation_lack\n",
       "0    50322\n",
       "1     7908\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.motivation_lack.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working conditions: demand-control model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert work characteristics questions to numeric\n",
    "for i in [26, 27, 28, 29, 34, 35]:\n",
    "    df_filtered[f\"ep0{i}_\"] = df_filtered[f\"ep0{i}_\"].replace(\n",
    "        {\n",
    "            \"Strongly agree\": 5,\n",
    "            \"Agree\": 4,\n",
    "            \"Don't know\": 3,\n",
    "            \"Disagree\": 2,\n",
    "            \"Strongly disagree\": 1,\n",
    "        }\n",
    "    )\n",
    "    df_filtered[f\"ep0{i}_\"] = pd.to_numeric(df_filtered[f\"ep0{i}_\"], errors=\"coerce\")\n",
    "    df_filtered = df_filtered.dropna(subset=[f\"ep0{i}_\"])\n",
    "\n",
    "for i in range(30, 34):\n",
    "    df_filtered[f\"ep0{i}_\"] = df_filtered[f\"ep0{i}_\"].replace(\n",
    "        {\n",
    "            \"Strongly agree\": 1,\n",
    "            \"Agree\": 2,\n",
    "            \"Don't know\": 3,\n",
    "            \"Disagree\": 4,\n",
    "            \"Strongly disagree\": 5,\n",
    "        }\n",
    "    )\n",
    "    df_filtered[f\"ep0{i}_\"] = pd.to_numeric(df_filtered[f\"ep0{i}_\"], errors=\"coerce\")\n",
    "    df_filtered = df_filtered.dropna(subset=[f\"ep0{i}_\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate groups above and below median for job demands and control\n",
    "df_filtered[\"job_demands\"] = 0\n",
    "df_filtered[\"job_control\"] = 0\n",
    "\n",
    "for i in range(len(df_filtered)):\n",
    "    if ((df_filtered[\"ep027_\"][i] + df_filtered[\"ep028_\"][i]) / 2) > (\n",
    "        (df_filtered[\"ep027_\"] + df_filtered[\"ep028_\"]) / 2\n",
    "    ).median():\n",
    "        df_filtered[\"job_demands\"][i] = 1\n",
    "    if (\n",
    "        (df_filtered[\"ep029_\"][i] + df_filtered[\"ep030_\"][i] + df_filtered[\"ep031_\"][i])\n",
    "        / 3\n",
    "    ) > (\n",
    "        (df_filtered[\"ep029_\"] + df_filtered[\"ep030_\"] + df_filtered[\"ep031_\"]) / 3\n",
    "    ).median():\n",
    "        df_filtered[\"job_control\"][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categories of jobs - passive, active, high strain, low strain\n",
    "df_filtered[\"job_passive\"] = 0\n",
    "df_filtered[\"job_active\"] = 0\n",
    "df_filtered[\"job_high_strain\"] = 0\n",
    "df_filtered[\"job_low_strain\"] = 0\n",
    "\n",
    "for i in range(len(df_filtered)):\n",
    "    if df_filtered[\"job_demands\"][i] == 0 and df_filtered[\"job_control\"][i] == 0:\n",
    "        df_filtered[\"job_passive\"][i] = 1\n",
    "    elif df_filtered[\"job_demands\"][i] == 1 and df_filtered[\"job_control\"][i] == 1:\n",
    "        df_filtered[\"job_active\"][i] = 1\n",
    "    elif df_filtered[\"job_demands\"][i] == 1 and df_filtered[\"job_control\"][i] == 0:\n",
    "        df_filtered[\"job_high_strain\"][i] = 1\n",
    "    else:\n",
    "        df_filtered[\"job_low_strain\"][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add job recognition, prospects, insecurity\n",
    "df_filtered[\"job_poor_recognition\"] = 0\n",
    "df_filtered[\"job_poor_prospects\"] = 0\n",
    "df_filtered[\"job_insecurity\"] = 0\n",
    "\n",
    "for i in range(len(df_filtered)):\n",
    "    if ((df_filtered[\"ep032_\"][i] + df_filtered[\"ep033_\"][i]) / 2) > (\n",
    "        (df_filtered[\"ep032_\"] + df_filtered[\"ep033_\"]) / 2\n",
    "    ).median():\n",
    "        df_filtered[\"job_poor_recognition\"][i] = 1\n",
    "    if df_filtered[\"ep034_\"][i] > df_filtered[\"ep034_\"].median():\n",
    "        df_filtered[\"job_poor_prospects\"][i] = 1\n",
    "    if df_filtered[\"ep035_\"][i] > df_filtered[\"ep035_\"].median():\n",
    "        df_filtered[\"job_insecurity\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16458 unique individuals in our sample.\n",
      "By wave couples:\n",
      "wave_couple\n",
      "w12     4861\n",
      "w24     8094\n",
      "w45    10341\n",
      "w56    10397\n",
      "w68     8790\n",
      "Name: mergeid, dtype: int64\n",
      "By wave:\n",
      "wave\n",
      "1    2861\n",
      "2    4431\n",
      "4    6752\n",
      "5    8253\n",
      "6    8006\n",
      "8    3052\n",
      "Name: mergeid, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unique individuals\n",
    "print(f\"There are {df_filtered.mergeid.nunique()} unique individuals in our sample.\")\n",
    "print(f\"By wave couples:\")\n",
    "print(df_filtered.groupby(\"wave_couple\").mergeid.nunique())\n",
    "print(f\"By wave:\")\n",
    "print(df_filtered.groupby(\"wave\").mergeid.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(\n",
    "    \"/Users/alexandralugova/Documents/GitHub/MH-old-workers/data/datasets/data_clean.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
